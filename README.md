# 90DaysOfDataOps

Here goes something else! 

I have been hearing the term DataOps amongst the world of Generative AI, Machine Learning, Deep Learning and Data Engineerings so I wanted to get my hands dirty and try something out in this field. 

If you are new here, I started a project called #90DaysOfDevOps which has ran for over 3 seasons talking about all things DevOps and DevSecOps. This project spanned GitHub, a website, and YouTube and seemed to resonate with many in the community. That project merely started as a me learning in public and getting to grips with the fundamental areas of DevOps. I want to take this same approach here with DataOps. 

I have found a fantastic resource that I am starting with (Data Alchemy)[https://www.skool.com/data-alchemy] which has around 30k members and this stems from some fantastic resources from (Dave Ebbelaar)[https://www.youtube.com/@daveebbelaar] 

I want to be very clear here! This project #90DaysOfDataOps as with #90DaysOfDevOps is in no way for my own financial gain. It is a community learning resource but I want to affiliate the hard work by the people in the community within this. There will be many people creating fantastic learning material across many different genres of Data that I want to bring to one place. 

Day 1 - Learning the basics of DataOps 

I am going to start writing my notes here and then later on we will get into a more project framework later on. 

The Data Alchemy course starts by talking about the "The art and science of transforming data into information" and how this high level description could span many different roles. I am not looking for a role but just looking to bolster my learning journey. 

There are then 7 Steps defined that I am sure we are going to get into more detail on: 

- Set up an environment
- Python (We have some great foundational content on Python within the #90DaysOfDevOps this is a good chance to review and update this content).
- Git (We also have a section on this within the #90DaysOfDevOps project, another one to review and update).
- Hands-On scenarios and projects
- Specialisation
- Continous Learning
- Monetise (As mentioned this is not my goal so I wont be covering this step)

I am excited to get started here and continue the learning journey. I will be adding resources and notes here. 

A lot of the content I am seeing here around DataOps is focused on using Python, I am going to try and stick with Golang which does seem to be gaining traction due to performance, concurrency and simpliciy so think we can make that work. 

# ðŸ“Œ DataOps 90-Day Learning Plan

## âœ… Introduction to DataOps & Setting Up Your Environment (Days 1-5)
- [ ] **Day 1**: What is DataOps? Principles, Goals & How It Differs from DevOps & Data Engineering.
- [ ] **Day 2**: The DataOps Lifecycle â€“ From Data Ingestion to Analytics.
- [ ] **Day 3**: Overview of DataOps Tools â€“ Airflow, dbt, Kafka, Great Expectations, etc.
- [ ] **Day 4**: Setting Up a DataOps Workstation â€“ Install Golang, Git, Terraform, Airflow, Kafka, Prometheus, etc.
- [ ] **Day 5**: Hands-on: Create & test a simple ETL pipeline with Golang.

## âœ… DataOps Culture, Agile Workflows & Team Collaboration (Days 6-9)
- [ ] **Day 6**: Agile & DevOps for Data â€“ Applying Scrum, Kanban, and CI/CD to Data Workflows.
- [ ] **Day 7**: Collaboration Between Data Engineers, Analysts & Scientists.
- [ ] **Day 8**: DataOps Maturity Model â€“ Evaluating Your Organization's Readiness.
- [ ] **Day 9**: Hands-on: Using Jira/Trello for DataOps Project Management.

## âœ… Version Control & Reproducibility in Data Workflows (Days 10-13)
- [ ] **Day 10**: Why Version Control is Critical for Data Engineering.
- [ ] **Day 11**: Git Basics â€“ Branching, Merging, Best Practices.
- [ ] **Day 12**: Data Version Control (DVC) â€“ Tracking Dataset Changes.
- [ ] **Day 13**: Hands-on: Set Up a Git + DVC Repo & Version a Dataset.

## âœ… CI/CD for Data Pipelines: Automating Data Deployments (Days 14-17)
- [ ] **Day 14**: What is CI/CD in DataOps? Benefits (Use GitHub Actions for CI/CD).
- [ ] **Day 15**: Writing Tests for Data Pipelines (Unit, Integration & Schema Tests).
- [ ] **Day 16**: Hands-on: CI/CD Pipeline for a DataOps Workflow.
- [ ] **Day 17**: Monitoring & Debugging CI/CD Failures in DataOps.

## âœ… Infrastructure as Code (IaC) for Data Workflows (Days 18-21)
- [ ] **Day 18**: Introduction to Infrastructure as Code (IaC).
- [ ] **Day 19**: Managing Data Infrastructure with Terraform.
- [ ] **Day 20**: Deploying Data Pipelines on Kubernetes.
- [ ] **Day 21**: Hands-on: Deploy Apache Airflow on Kubernetes.

## âœ… Data Security & Access Management (Days 22-25)
- [ ] **Day 22**: Security Best Practices in DataOps.
- [ ] **Day 23**: Role-Based Access Control (RBAC) & Identity Management.
- [ ] **Day 24**: Hands-on: Implement Access Controls in a DataOps Pipeline.
- [ ] **Day 25**: Threat Detection & Incident Response in Data Workflows.

## âœ… Observability, Monitoring & Alerting in DataOps (Days 26-29)
- [ ] **Day 26**: Why Observability Matters in DataOps.
- [ ] **Day 27**: Logging Best Practices for Data Pipelines.
- [ ] **Day 28**: Hands-on: Set Up Grafana for Real-Time Data Monitoring.
- [ ] **Day 29**: Alerting & Anomaly Detection for Data Pipelines.

## âœ… Generative AI & DataOps (Days 30-34)
- [ ] **Day 30**: Introduction to Generative AI in DataOps â€“ Use Cases & Trends.
- [ ] **Day 31**: AI Agents & Multi-Agent Collaboration (AutoGPT, LangChain).
- [ ] **Day 32**: Model Control Plane (MCP) â€“ Managing Multiple AI Models.
- [ ] **Day 33**: Hands-on: Deploy an AI-Powered DataOps Workflow.
- [ ] **Day 34**: AI-Powered Data Pipeline Automation (Prompt-Based ETL).

## âœ… Data Engineering Fundamentals (Days 35-39)
- [ ] **Day 35**: ETL vs. ELT â€“ Understanding Data Processing Strategies.
- [ ] **Day 36**: Batch vs. Streaming Pipelines.
- [ ] **Day 37**: Hands-on: Implement an ETL Pipeline with Airflow & dbt.
- [ ] **Day 38**: Automating Data Transformations & Workflow Scheduling.
- [ ] **Day 39**: Mini-project: Build an End-to-End Data Engineering Pipeline.

## âœ… Data Storage & Warehousing (Days 40-44)
- [ ] **Day 40**: SQL vs. NoSQL â€“ Choosing the Right Storage Solution.
- [ ] **Day 41**: Data Lakes vs. Data Warehouses â€“ When to Use Each.
- [ ] **Day 42**: Hands-on: Set Up a Data Warehouse with BigQuery or Snowflake.
- [ ] **Day 43**: Performance Optimization & Cost Reduction Strategies.
- [ ] **Day 44**: Mini-project: Design a Scalable Data Storage Solution.

## âœ… Data Quality & Testing (Days 45-49)
- [ ] **Day 45**: Importance of Data Quality & Testing.
- [ ] **Day 46**: Introduction to Great Expectations & Monte Carlo.
- [ ] **Day 47**: Hands-on: Write & Automate Data Quality Tests.
- [ ] **Day 48**: Monitoring & Alerting for Data Quality Failures.
- [ ] **Day 49**: Mini-project: Implement a Data Quality Framework.

## âœ… Pipeline Orchestration (Days 50-54)
- [ ] **Day 50**: Introduction to Orchestration â€“ Airflow vs. Prefect vs. Dagster.
- [ ] **Day 51**: Hands-on: Build a DAG in Apache Airflow.
- [ ] **Day 52**: Automating Pipeline Scheduling & Failure Handling.
- [ ] **Day 53**: Debugging & Scaling Data Pipelines.
- [ ] **Day 54**: Mini-project: Deploy a Production-Grade Orchestration Workflow.

## âœ… Streaming Data & Real-Time Processing (Days 55-59)
- [ ] **Day 55**: Introduction to Real-Time Data Processing.
- [ ] **Day 56**: Hands-on: Implement a Kafka Streaming Pipeline.
- [ ] **Day 57**: Windowing, Event Time & Watermarking in Streaming Data.
- [ ] **Day 58**: Fault Tolerance & Performance Optimization.
- [ ] **Day 59**: Mini-project: Deploy a Kafka Streaming Pipeline for Real-Time Fraud Detection.

## âœ… ML & AI in DataOps (Days 60-64)
- [ ] **Day 60**: MLOps Basics â€“ Managing Models in Production.
- [ ] **Day 61**: Feature Stores & Model Deployment.
- [ ] **Day 62**: Hands-on: Deploy an ML Model using MLflow.
- [ ] **Day 63**: Automating Model Retraining & Monitoring.
- [ ] **Day 64**: Mini-project: End-to-End MLOps Pipeline.

## âœ… Cloud DataOps, Governance & Metadata Management (Days 65-71)
- [ ] **Day 65**: Cloud DataOps â€“ AWS, GCP, and Azure DataOps Services.
- [ ] **Day 66**: Data Governance â€“ GDPR, HIPAA Compliance, Access Control.
- [ ] **Day 67**: Data Lineage â€“ Tracking Data Changes & Flow.
- [ ] **Day 68**: Hands-on: Implement Data Governance Policies.
- [ ] **Day 69**: Metadata Management â€“ Apache Atlas, OpenLineage.
- [ ] **Day 70**: Automated Data Documentation â€“ Amundsen, DataHub.
- [ ] **Day 71**: Hands-on: Implement Metadata Management in DataOps.

## âœ… Synthetic Data & Cost Optimization (Days 72-75)
- [ ] **Day 72**: Introduction to Synthetic Data & Use Cases.
- [ ] **Day 73**: Data Masking & Privacy-Preserving Analytics.
- [ ] **Day 74**: Hands-on: Generate Synthetic Data for Testing.
- [ ] **Day 75**: Query Performance Tuning & Storage Cost Reduction.

## âœ… Future Trends & Emerging Technologies (Days 76-80)
- [ ] **Day 76**: AI-Driven DataOps â€“ Automation & Intelligence.
- [ ] **Day 77**: Data Contracts & API-First DataOps.
- [ ] **Day 78**: Hands-on: Implement a Data Contract for a Pipeline.
- [ ] **Day 79**: Exploring Serverless DataOps â€“ When & How to Use It.
- [ ] **Day 80**: Scaling & Performance Optimization in DataOps.

## âœ… Final Integration & Capstone Project (Days 81-90)
### ðŸ”¹ Recap & Best Practices (Days 81-82)
- [ ] **Day 81**: Reviewing Best Practices Across DataOps.
- [ ] **Day 82**: Designing an End-to-End DataOps System.

### ðŸ”¹ Capstone Implementation (Days 83-86)
- [ ] **Day 83**: Hands-on: Implement a Full DataOps Workflow.
- [ ] **Day 84**: Testing, Debugging & Documentation.
- [ ] **Day 85**: Scaling & Performance Optimization in DataOps.
- [ ] **Day 86**: DataOps Workflow Review & Optimization.

### ðŸ”¹ Presentation & Next Steps (Days 87-90)
- [ ] **Day 87**: Capstone Presentation.
- [ ] **Day 88**: Career & Future Learning Paths.
- [ ] **Day 89**: Personal Roadmap & Open-Source Contribution.
- [ ] **Day 90**: Wrap Up & Final Reflections.
